{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0643112e-3cbf-4fd0-afb8-54aeffe3fe93"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/EAN.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/0_intro_ml.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm Chains and Pipelines\n",
    "\n",
    "\n",
    "## Instructors:\n",
    "\n",
    ">Leonardo A. Espinosa, PhD. Instructor.\n",
    "(*email*: laespinosa@universidadean.edu.co)\n",
    "\n",
    "> Ruben D. Acosta, MSc. Instructor.\n",
    "(*email*:  rdacostav@universidadean.edu.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "39576c63-298e-4a6d-ac14-aba4e1c4d006"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1\\. <a href=\"#/25/1\">Algorithm Chains and Pipelines</a>\n",
    "\n",
    "   * <a href=\"#/31/1\">The *Pipeline* Class</a>.\n",
    "   * <a href=\"#/33/1\">An example of Information Leakage</a>.\n",
    "   * <a href=\"#/35/1\">The General Pipeline Interface</a>\n",
    "   \n",
    "2\\. <a href=\"#/42/1\">Conclusions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6f63fa54-d56f-434f-943f-be2a6f0e3d05"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm Chains and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2564c16a-8437-4ca7-a63d-82789db10af7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipeline and GridSearchCV Classes\n",
    "\n",
    "A machine learning problem usually includes:\n",
    "\n",
    "* Scaling the data and combining features by hand. Learning features using unsupervised machine learning.\n",
    "\n",
    "* Chaining together many different processing steps and machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "from matplotlib import rc\n",
    "font = {'family' : 'monospace', 'weight' : 'bold', 'size'   : 25}\n",
    "rc('font', **font) \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.rcParams['lines.linewidth'] = 5.0\n",
    "plt.rcParams['lines.markersize'] = 15.0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "46bd3e13-ee52-495d-847d-873607277dbc"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load and split the data\n",
    "cancer = load_breast_cancer() \n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "# compute minimum and maximum on the training data\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# rescale the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "svm = SVC()\n",
    "\n",
    "# learn an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "#svm.fit(X_train, y_train)\n",
    "\n",
    "# scale the test data and score the scaled data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "43e9b8d9-0f53-40f3-b8d9-c5b60fad5133"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Selection with Preprocessing\n",
    "\n",
    "* Find the best parameters for *SVC* using *GridSearch* $\\to$ Just add the grid parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c4ec721c-2942-42d9-a1e4-2c621eb12fb9"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ae20116a-eb68-4cb6-86d0-819b0626bdce"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ERROR $\\Rightarrow$ The grid search is done using the scaled data!\n",
    "* But the scaled data was done on the whole data in the training set.\n",
    "* For each split in the cross-validation some information is leaked from the test set into the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e9d10ac4-5924-4b8d-889d-e9119c8444ea"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/04-pipelines.png\" alt=\"Drawing\" style=\"width:1500px\" align=\"middle\"/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "638e3833-24a8-4b69-af6b-e29c79f63b2a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The *Pipeline* Class\n",
    "\n",
    "You can create a *workflow* for training and classification after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "354618e2-6431-4f78-b0d9-f9d6c4e27cbb"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e7bce1ec-9fd1-4d0b-b65c-51e051842ca2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines in Grid Searches\n",
    "Pipelines and grid search are combined in the same way that any estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eea136b7-17a2-4402-a3f0-970b981627ee"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "284aeb6d-45b2-4121-a0c7-bbe35a220f02"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Leakage\n",
    "\n",
    "Example adapted from *The Elements of Statistical Learning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cd605493-3e21-4ca0-8823-eb6f63af6081"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f4388a81-cd22-49d1-be8e-9f9169051761"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "select = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(\"X_selected.shape: {}\".format(X_selected.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6aad3d75-e549-409c-83c3-a651df52e65f"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "print(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\n",
    "    np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4e8f5a45-9cc5-4f07-85c2-7e65e24dfbbd"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$R^2$ computed by cross-validation is 0.91 $\\to$ a very good model. **ERROR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b884e1ce-ada2-4065-aa11-dd0d783166fa"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proper model with cross-validation using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cd546b22-a23d-4dc4-8962-54c9a14ea0af"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression, \n",
    "                                             percentile=5)),\n",
    "               (\"ridge\", Ridge())])\n",
    "print(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\n",
    "np.mean(cross_val_score(pipe, X, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "05f5a4ab-7da0-4269-9aaa-9c99de3ce3f0"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a negative $R^2$ score, indicating a very poor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5a3c94ba-305b-4a76-8d8e-b3f2d755fc05"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipeline Creation with make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "54f0b15e-8041-463e-9682-a3cbee3af133"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# standard syntax\n",
    "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\n",
    "\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "224ccbd9-f64c-4f64-9454-b8ed8e81030e"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, the step names are just lowercase versions of the class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "54ac1cd8-248d-4742-a7cb-985267a418d3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6e73c082-189f-456b-9eca-a27a3823930d"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note:** In this case it might be better to use the Pipeline construction with explicit name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b4f8d087-224e-4b94-a836-098b2bbcae17"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid-Searching Preprocessing Steps and Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b3f50ad3-db86-455a-9030-3a812b3c0727"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,random_state=0)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(),PolynomialFeatures(),Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3a5d1f05-c830-4a15-84b9-b67f6a804eb8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6bf31f5d-6aa9-476b-9313-54d1c8fa73ad"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n",
    "            vmin=0, cmap=\"viridis\")\n",
    "plt.xlabel(\"ridge__alpha\")\n",
    "plt.ylabel(\"polynomialfeatures__degree\")\n",
    "plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\n",
    "plt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n",
    "param_grid['polynomialfeatures__degree'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "95ee1be4-4a90-4463-9b42-9c86c8415046"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "57f23b1b-15c8-460b-82bd-87a60624dc34"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exhaustive Grid Search\n",
    "* Again, it is not a recomendable strategy.\n",
    "* If your dataset is large you can try on a random subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f6a71069-9e93-4a7d-9b0c-7e3a77501f84"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\n",
    "     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    {'classifier': [RandomForestClassifier(n_estimators=100)],\n",
    "     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "be5e8221-9a6f-4c72-8fc9-c6fd552dbc1e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "55070123-ac6f-4896-94de-d50a98dfc37b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomized Parameter Optimization\n",
    "\n",
    "`RandomizedSearchCV` implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values.\n",
    "\n",
    "Advantages:\n",
    "* A budget can be chosen independent of the number of parameters and possible values.\n",
    "* Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "See: Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# get some data\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# build a classifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='elasticnet',fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'average': [True, False],\n",
    "              'l1_ratio': stats.uniform(0, 1),\n",
    "              'alpha': loguniform(1e-4, 1e0)}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# use a full grid over all parameters\n",
    "param_grid = {'average': [True, False],\n",
    "              'l1_ratio': np.linspace(0, 1, num=10),\n",
    "              'alpha': np.power(10, np.arange(-4, 1, dtype=float))}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GridSearchCV 2.0 -- New and Improved\n",
    "\n",
    "Cutting edge hyperparameter tuning techniques (bayesian optimization, early stopping, distributed execution) can provide significant speedups over grid search and random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/tune+sklearn.png\" style=\"width:800px\">\n",
    "    https://github.com/ray-project/tune-sklearn\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tune: Scalable Hyperparameter Tuning\n",
    "\n",
    "Tune is a Python library for experiment execution and hyperparameter tuning at any scale (https://docs.ray.io/en/latest/tune.html). Core features:\n",
    "\n",
    "* Launch a multi-node distributed hyperparameter sweep in less than 10 lines of code.\n",
    "\n",
    "* Supports any machine learning framework, including PyTorch, XGBoost, MXNet, and Keras.\n",
    "\n",
    "* Natively integrates with optimization libraries such as HyperOpt, Bayesian Optimization, and Facebook Ax.\n",
    "\n",
    "* Choose among scalable algorithms such as Population Based Training (PBT), Vizier’s Median Stopping Rule, HyperBand/ASHA.\n",
    "\n",
    "* Visualize results with TensorBoard.\n",
    "\n",
    "* Move your models from training to serving on the same infrastructure with Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here’s what tune-sklearn has to offer:\n",
    "\n",
    "* Consistency with Scikit-Learn API: tune-sklearn is a drop-in replacement for GridSearchCV and RandomizedSearchCV, so you only need to change less than 5 lines in a standard Scikit-Learn script to use the API.\n",
    "* Modern hyperparameter tuning techniques: tune-sklearn is the only Scikit-Learn interface that allows you to easily leverage Bayesian Optimization, HyperBand, and other optimization techniques by simply toggling a few parameters.\n",
    "* Framework support: tune-sklearn is used primarily for tuning Scikit-Learn models, but it also supports and provides examples for many other frameworks with Scikit-Learn wrappers such as Skorch (Pytorch), KerasClassifiers (Keras), and XGBoostClassifiers (XGBoost).\n",
    "* Scale up: Tune-sklearn leverages Ray Tune, a library for distributed hyperparameter tuning, to efficiently and transparently parallelize cross validation on multiple cores and even multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Here we explore the pipeline class.\n",
    "\n",
    "* Create a pipeline class combining one scaler and one model.\n",
    "* Run a gridsearch with this pipeline on your dataset or the wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ca50e6f1-b3eb-4396-8e0a-be52b069f162"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions\n",
    "* Split into training data for model building, validation data for model and parameter selection, and test data for model evaluation.\n",
    "* The Pipeline class is a general-purpose tool to chain together multiple processing steps in a machine learning workflow.\n",
    "* Choosing the right combination of feature extraction, preprocessing, and models is somewhat of an art, and often requires some trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "de02f45b-586f-4b55-8b9b-04319b0fcb4f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tips for parameter search\n",
    "\n",
    "* **Specifying an objective metric**. For some applications, other scoring functions are better suited\n",
    "* **Specifying multiple metrics for evaluation**. `GridSearchCV` and `RandomizedSearchCV` allow specifying multiple metrics for the scoring parameter.\n",
    "* **Composite estimators and parameter spaces**. Use pipelines and caching transformers to avoid repeated computation. See `sklearn` manual.\n",
    "* **Parallelism**.\n",
    "* **Robustness to failure**. Setting `error_score=0` (or `=np.NaN`) will make the procedure robust to such failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fd25ebff-a98e-4734-a4f4-da2894548ca1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternatives to brute force parameter search\n",
    "\n",
    "* **Model specific cross-validation**. Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter. eg. `linear_model.RidgeCV`, `linear_model.LogisticRegressionCV`.\n",
    "* ** Information Criterion**. Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation). eg. `linear_model.LassoLarsIC`\n",
    "* **Out of Bag Estimates**. When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out. eg. `ensemble.RandomForestClassifier`, `ensemble.GradientBoostingClassifier`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_thats_all.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_metadata": {
   "author": "Leonardo Espinosa",
   "title": "MLPP: Supervised Learning"
  },
  "livereveal": {
   "overlay": "<div class='myheader'><h2 class='headertekst'>Elementos de Machine Learning y Deep Learning. Universidad EAN </h2><h3 ><a href='#/21/1'>(index)</a></h3></div>",
   "progress": true,
   "scroll": true,
   "theme": "serif",
   "transition": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
